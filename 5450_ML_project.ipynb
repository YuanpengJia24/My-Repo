{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wvNfmBOrMSq"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyw56qY24hVK"
      },
      "source": [
        "**Introduction to the Project**\n",
        "\n",
        "Welcome to the Credit Card Fraud Detection Analysis project!\n",
        "\n",
        "I have set out to analyze a comprehensive dataset of Credit Card Fraud  across the Europe in order to train and evaluate a model that can accurately predict and identify fraud in credit card transactions. By leveraging this dataset, which contains around 550,000 records, and I plan to train different Machine Learning model and then evaluate the best algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp2ALiHL8e24"
      },
      "source": [
        "**Data scource**\n",
        "\n",
        "The dataset, sourced from https://www.kaggle.com/datasets/nelgiriyewithana/credit-card-fraud-detection-dataset-2023/data, is a rich resource that includes anonymized variables representing various transaction attributes like time, location and so on. Since all variables are standardized, feature selection will be fair and just."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b__tnR4H8ovq"
      },
      "source": [
        "**Project design**\n",
        "\n",
        "In this project, I will explore various statistical and machine learning models, including different supervised and unsupervised approaches to analyse the factors that effect the severity of the car accident. I will first use unsupervised machine learning method to reduce the number of features. Then I will compare the result of different supervised machine learning models. I aim to facilitate the development of fraud detection algorithms and models to identify potentially fraudulent transactions.\n",
        "\n",
        "The framework of the project is below:\n",
        "\n",
        "Part 1: Data Cleaning and Preprocessing\n",
        "\n",
        "Part 2: Exploratory data analysis (EDA)\n",
        "\n",
        "Part 3: Machine Learning Modeling (Unsupervised and supervised)\n",
        "\n",
        "Part 4: Model Comparison and Conclusion\n",
        "\n",
        "Part 5: Challenges and Obstacles Faced\n",
        "\n",
        "Part 6: Further Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YChzsmTp4-vu"
      },
      "source": [
        "# Part 0 : Before Running⚠️\n",
        "\n",
        "Remember to upload your kaggle.json file (kaggle account required) to colab after you run the following channel. Or, you can directly download file from https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.kaggle.com%2Fdatasets%2Fsobhanmoosavi%2Fus-accidents and upload to colab.\n",
        "\n",
        "Alternately, if you don't want to run it, make sure to load it in Colab to have a complete view of our data visualization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u2wAVSu49k_"
      },
      "source": [
        "## Installing the Kaggle API in Colab\n",
        "\n",
        "The dataset we use from kaggle. If you wish to run this notebook, you can do following steps:\n",
        "\n",
        "\n",
        "First, grab your token from Kaggle.\n",
        "1. Navigate to https://www.kaggle.com.\n",
        "2. Then go to the [Account tab of your user profile](https://www.kaggle.com/me/account).\n",
        "3. select Create API Token. This will trigger the download of kaggle.json, a file containing your API credentials.\n",
        "\n",
        "Then run the cell below to upload kaggle.json to your Colab runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4vbeuyC6dB2",
        "outputId": "7d1f4c6a-c92f-4d8c-f208-ba2e7460a4f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTt0fph96umI",
        "outputId": "b8ed8be9-bcf5-4504-b8df-8fa4785d0148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiHN6LPd7UAl"
      },
      "outputs": [],
      "source": [
        "# Create the kaggle directory\n",
        "# (NOTE: Do NOT run this cell more than once unless restarting kernel)\n",
        "!mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhv3H44D7YnW"
      },
      "outputs": [],
      "source": [
        "# Read the uploaded kaggle.json file\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sjyyKXP7cuo",
        "outputId": "90745b20-3de4-4f6a-df08-315c5442354f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Downloading us-accidents.zip to /content',\n",
              " '',\n",
              " '  0% 0.00/653M [00:00<?, ?B/s]',\n",
              " '  1% 8.00M/653M [00:00<00:08, 83.1MB/s]',\n",
              " '  3% 19.0M/653M [00:00<00:06, 100MB/s] ',\n",
              " '  5% 31.0M/653M [00:00<00:05, 111MB/s]',\n",
              " '  7% 47.0M/653M [00:00<00:04, 131MB/s]',\n",
              " '  9% 60.0M/653M [00:00<00:05, 120MB/s]',\n",
              " ' 11% 72.0M/653M [00:00<00:05, 113MB/s]',\n",
              " ' 13% 83.0M/653M [00:00<00:08, 68.1MB/s]',\n",
              " ' 14% 92.0M/653M [00:01<00:08, 70.9MB/s]',\n",
              " ' 16% 102M/653M [00:01<00:07, 78.1MB/s] ',\n",
              " ' 17% 111M/653M [00:01<00:07, 77.8MB/s]',\n",
              " ' 18% 120M/653M [00:01<00:07, 74.6MB/s]',\n",
              " ' 21% 135M/653M [00:01<00:05, 93.4MB/s]',\n",
              " ' 22% 145M/653M [00:01<00:06, 79.0MB/s]',\n",
              " ' 25% 162M/653M [00:01<00:05, 99.0MB/s]',\n",
              " ' 28% 180M/653M [00:01<00:04, 119MB/s] ',\n",
              " ' 30% 193M/653M [00:02<00:03, 123MB/s]',\n",
              " ' 32% 206M/653M [00:02<00:03, 124MB/s]',\n",
              " ' 34% 222M/653M [00:02<00:03, 132MB/s]',\n",
              " ' 36% 236M/653M [00:02<00:03, 116MB/s]',\n",
              " ' 39% 252M/653M [00:02<00:03, 129MB/s]',\n",
              " ' 41% 265M/653M [00:02<00:03, 123MB/s]',\n",
              " ' 43% 278M/653M [00:02<00:03, 103MB/s]',\n",
              " ' 44% 289M/653M [00:03<00:03, 98.9MB/s]',\n",
              " ' 46% 299M/653M [00:03<00:04, 78.8MB/s]',\n",
              " ' 47% 308M/653M [00:03<00:05, 70.1MB/s]',\n",
              " ' 48% 316M/653M [00:03<00:05, 70.7MB/s]',\n",
              " ' 50% 325M/653M [00:03<00:04, 73.7MB/s]',\n",
              " ' 51% 333M/653M [00:03<00:04, 75.1MB/s]',\n",
              " ' 52% 342M/653M [00:03<00:04, 79.3MB/s]',\n",
              " ' 54% 354M/653M [00:03<00:03, 90.5MB/s]',\n",
              " ' 56% 363M/653M [00:04<00:03, 82.9MB/s]',\n",
              " ' 57% 372M/653M [00:04<00:03, 82.5MB/s]',\n",
              " ' 58% 382M/653M [00:04<00:03, 85.0MB/s]',\n",
              " ' 60% 393M/653M [00:04<00:02, 92.8MB/s]',\n",
              " ' 62% 403M/653M [00:04<00:02, 94.6MB/s]',\n",
              " ' 63% 413M/653M [00:04<00:03, 82.4MB/s]',\n",
              " ' 65% 422M/653M [00:04<00:03, 79.7MB/s]',\n",
              " ' 68% 441M/653M [00:04<00:02, 103MB/s] ',\n",
              " ' 69% 452M/653M [00:05<00:02, 101MB/s]',\n",
              " ' 71% 464M/653M [00:05<00:01, 107MB/s]',\n",
              " ' 73% 475M/653M [00:05<00:01, 102MB/s]',\n",
              " ' 74% 485M/653M [00:05<00:02, 82.7MB/s]',\n",
              " ' 76% 494M/653M [00:05<00:01, 84.3MB/s]',\n",
              " ' 77% 503M/653M [00:05<00:01, 85.9MB/s]',\n",
              " ' 78% 512M/653M [00:05<00:01, 85.0MB/s]',\n",
              " ' 80% 521M/653M [00:05<00:01, 83.7MB/s]',\n",
              " ' 81% 530M/653M [00:06<00:01, 73.0MB/s]',\n",
              " ' 82% 538M/653M [00:06<00:01, 74.6MB/s]',\n",
              " ' 84% 546M/653M [00:06<00:01, 76.4MB/s]',\n",
              " ' 85% 555M/653M [00:06<00:01, 79.8MB/s]',\n",
              " ' 86% 563M/653M [00:06<00:01, 78.5MB/s]',\n",
              " ' 88% 572M/653M [00:06<00:01, 81.0MB/s]',\n",
              " ' 89% 582M/653M [00:06<00:01, 73.1MB/s]',\n",
              " ' 90% 590M/653M [00:06<00:01, 63.6MB/s]',\n",
              " ' 92% 601M/653M [00:07<00:01, 49.2MB/s]',\n",
              " ' 93% 610M/653M [00:07<00:00, 56.9MB/s]',\n",
              " ' 95% 621M/653M [00:07<00:00, 66.8MB/s]',\n",
              " ' 97% 632M/653M [00:07<00:00, 75.5MB/s]',\n",
              " ' 98% 641M/653M [00:07<00:00, 75.7MB/s]',\n",
              " ' 99% 649M/653M [00:07<00:00, 71.3MB/s]',\n",
              " '',\n",
              " '100% 653M/653M [00:07<00:00, 86.4MB/s]']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Download dataset\n",
        "!!kaggle datasets download -d sobhanmoosavi/us-accidents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMwNumBY8HUr",
        "outputId": "4f8bdef0-86b5-423f-d387-f0ff96306dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/us-accidents.zip\n",
            "  inflating: US_Accidents_March23.csv  \n"
          ]
        }
      ],
      "source": [
        "# Unzip folder in Colab content folder\n",
        "!unzip /content/us-accidents.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNytm1Jn8u73"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import json\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import re\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j25GBJyPxP8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aac7d0e-4d95-4497-b689-def8635350dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n",
            "cpu\n",
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "from skimage import io, transform\n",
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "torch.manual_seed(42) # For grading consistency\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.__version__)\n",
        "print(device)\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOZS1owX9cCF"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "We are loading the US_accident csv file that we downloaded from Kaggle.\n",
        "\n",
        "Load `US_Accidents_Mar23.csv` as `df_accident`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6zAq3IS8dTd"
      },
      "outputs": [],
      "source": [
        "#read the dataset in\n",
        "df_accident = pd.read_csv('US_Accidents_March23.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji41O3ZO9t-d"
      },
      "outputs": [],
      "source": [
        "#sneak peak of the dataset\n",
        "df_accident.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJtle21F-rYH"
      },
      "outputs": [],
      "source": [
        "#number of rows and columns in the dataset\n",
        "print('The Dataset Contains, Rows: {:,d} & Columns: {}'.format(df_accident.shape[0], df_accident.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRguZ2Hh_ND3"
      },
      "outputs": [],
      "source": [
        "df_dtypes = df_accident.dtypes.reset_index()\n",
        "df_dtypes.columns = [\"Column Name\", \"Column Type\"]\n",
        "df_dtypes#check each column's variable type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJhlLd41-GCV"
      },
      "source": [
        "# Part 1: Data Cleaning and Preprocessing\n",
        "\n",
        "We are going to clean and preprocess our `df_accident` in this part.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0w5m0QH-UjB"
      },
      "outputs": [],
      "source": [
        "# convert the Start_Time & End_Time Variable into Datetime objects and create two new columns\n",
        "df_accident['start_time'] = pd.to_datetime(df_accident.Start_Time,errors='coerce')\n",
        "df_accident['end_time'] = pd.to_datetime(df_accident.End_Time,errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iNYfeZ-lHsl"
      },
      "outputs": [],
      "source": [
        "# check the number of rows with null values in each column\n",
        "null_counts = df_accident.isnull().sum()\n",
        "print(null_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHpUGJ-ncG2U"
      },
      "source": [
        "In order to avoid too much observations and create extremely long run times for the system, we decided to select 50000 rows randomly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNWCeaF6_g0_"
      },
      "outputs": [],
      "source": [
        "#drop N/A values and any Unamed columns in the dataframe\n",
        "clean_df = df_accident.dropna()\n",
        "#randomly select 50000 observations from the dataset to avoid extremely long run time for future models\n",
        "clean_df = clean_df.sample(n=50000, random_state=42)\n",
        "#clean_df = clean_df.drop('Unnamed: 0', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5wRe_oIBAb-"
      },
      "outputs": [],
      "source": [
        "#check the number of states in the dataset\n",
        "states = clean_df.State.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiJ6nyXqtQFl"
      },
      "outputs": [],
      "source": [
        "# Extract year, month, day, hour and weekday of both start time and end time and create new columns in the dataset\n",
        "clean_df['Year'] = clean_df['start_time'].dt.year\n",
        "clean_df['Month'] = clean_df['start_time'].dt.strftime('%b')\n",
        "clean_df['Day'] = clean_df['start_time'].dt.day\n",
        "clean_df['Hour'] = clean_df['start_time'].dt.hour\n",
        "clean_df['Weekday'] = clean_df['start_time'].dt.strftime('%a')\n",
        "clean_df['Year_end'] = clean_df['end_time'].dt.year\n",
        "clean_df['Month_end'] = clean_df['end_time'].dt.strftime('%b')\n",
        "clean_df['Day_end'] = clean_df['end_time'].dt.day\n",
        "clean_df['Hour_end'] = clean_df['end_time'].dt.hour\n",
        "clean_df['Weekday_end'] = clean_df['end_time'].dt.strftime('%a')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5MujcWntAD0"
      },
      "outputs": [],
      "source": [
        "# Extract the amount of time in the unit of minutes for each accident, round to the nearest integer\n",
        "td='Time_Duration(min)'\n",
        "clean_df[td]=round((clean_df['end_time']-clean_df['start_time'])/np.timedelta64(1,'m'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Neq4H_Vz2IgP"
      },
      "outputs": [],
      "source": [
        "#define the negative accident time as outliers\n",
        "outliers=clean_df[td]<=0\n",
        "\n",
        "# Set outlying accident time to NAN\n",
        "clean_df[outliers] = np.nan\n",
        "\n",
        "# Drop rows with NaN values\n",
        "clean_df.dropna(subset=[td],axis=0,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxQOZkPlzUH3"
      },
      "source": [
        "Create a new column called `severe_accident`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpaGL_DmzSW2"
      },
      "outputs": [],
      "source": [
        "#divide accident's severity into two categories, if the severity is 1 or 2,\n",
        "#it will be categorized as not severe, else it is severe\n",
        "severity = lambda x: 'no' if x in [1, 2] else 'yes'\n",
        "\n",
        "# apply the lambda function to create a new column\n",
        "clean_df['severe_accident'] = clean_df['Severity'].apply(severity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvLrE8xdjYCM"
      },
      "source": [
        "# Part 2: Exploratory data analysis (EDA): Data Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "memMWItt3fnM"
      },
      "source": [
        "## Handling Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG1danmF3pAE"
      },
      "outputs": [],
      "source": [
        "missing_data = df_accident.isnull().sum()\n",
        "missing_data_percentage = (missing_data / len(df_accident)) * 100\n",
        "print(\"Missing Data Percentage: \\n\", missing_data_percentage)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH5jYqZ-3tXG"
      },
      "source": [
        "## Get Accident counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbrlkbybwxBl"
      },
      "outputs": [],
      "source": [
        "df_state_counts = pd.DataFrame(clean_df.groupby(['State'])['ID'].count())\n",
        "df_state_counts = df_state_counts.reset_index()\n",
        "df_state_counts = df_state_counts.rename(columns={'ID': 'total counts'})\n",
        "df_state_counts = df_state_counts.sort_values('total counts', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsmHv-gc9AIk"
      },
      "outputs": [],
      "source": [
        "df_state_counts #gives a clear view of which state has the most accidents and least accidents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xru3qSgAmTT1"
      },
      "source": [
        "## Identify which states have the most number of accidents with visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "174UED6-iyIb"
      },
      "outputs": [],
      "source": [
        "counts_by_states=[]\n",
        "for i in clean_df.State.unique():\n",
        "    counts_by_states.append(clean_df[clean_df['State']==i].count()['ID'])\n",
        "\n",
        "fig,ax = plt.subplots(figsize=(10,8))\n",
        "ax = sns.barplot(x=states, y=counts_by_states, ax=ax)\n",
        "ax.set(xlabel='State Names', ylabel='Number of Accidents', title = 'Numbers of Accidents in Different States')\n",
        "plt.xticks(rotation = 45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqiRs4ZPmj3v"
      },
      "source": [
        "## check for the top 5 weather conditions that can cause accidents to happen with visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwbYfaoxjEdE"
      },
      "outputs": [],
      "source": [
        "fig, ax=plt.subplots(figsize=(6,6))\n",
        "clean_df['Weather_Condition'].value_counts().sort_values(ascending=False).head(5).plot.bar(width=0.5,edgecolor='k',align='center',linewidth=2)\n",
        "plt.xlabel('Weather_Condition')\n",
        "plt.ylabel('Number of Accidents')\n",
        "ax.tick_params(labelsize=20)\n",
        "plt.title('5 Top Weather Condition for accidents')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGkDwQvhE99q"
      },
      "source": [
        "## Map visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-ww_QAaDH2J"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# We will limit the data to a smaller sample for faster visualization\n",
        "sample_df = df_accident.sample(5000)\n",
        "\n",
        "fig = px.scatter_geo(sample_df,\n",
        "                     lat='Start_Lat',\n",
        "                     lon='Start_Lng',\n",
        "                     color='Severity',\n",
        "                     hover_name='Description',\n",
        "                     size='Severity',\n",
        "                     scope='usa',\n",
        "                     title='Accident Locations and Severity in the United States')\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYagL56Upvy4"
      },
      "source": [
        "**Severity**: examine the relationship between accident severity and time duration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiGgYAOl9o23"
      },
      "outputs": [],
      "source": [
        "# Analyze accident severity\n",
        "sns.countplot(x='Severity', data=df_accident)\n",
        "plt.title(\"Accident Severity Distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aW4U-W7pvpg"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x='Severity', y='Time_Duration(min)', data=clean_df)\n",
        "plt.title('Accident Severity by Time Duration')\n",
        "plt.xlabel('Severity')\n",
        "plt.ylabel('Time Duration (min)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEcRAiluvHEt"
      },
      "source": [
        "## Distribution of accident severity by state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkb2vjGbuPQm"
      },
      "outputs": [],
      "source": [
        "clean_df['State'] = clean_df['State'].astype(str)\n",
        "state_severity = clean_df.groupby(['State', 'Severity']).size().reset_index(name='Counts')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "sns.set(font_scale=1.2)\n",
        "\n",
        "sns.scatterplot(x='State', y='Counts', hue='Severity', data=state_severity, s=100)\n",
        "plt.title('Distribution of Accident Severity by State', fontsize=24)\n",
        "plt.xlabel('State', fontsize=18)\n",
        "plt.ylabel('Counts', fontsize=18)\n",
        "plt.legend(title='Severity', fontsize=16, title_fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0YC0DPjwZZd"
      },
      "source": [
        "## Correlation between each variables(both independent and dependent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsQnPWs2vJhN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 10))\n",
        "corr = clean_df.corr().round(2)\n",
        "sns.heatmap(corr, cmap='coolwarm', annot=True, fmt='.2f', annot_kws={'fontsize': 12, 'fontweight': 'bold', 'color': 'black'}, ax=ax)\n",
        "ax.set_title('Correlation between Variables', fontsize=16, fontweight='bold')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoHUvd_XGhJl"
      },
      "source": [
        "# Part 3: Machine Learning Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWMO1mq4q3L6"
      },
      "source": [
        "## Unsupervised Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0laPgdVrSVJ"
      },
      "source": [
        "### **Hyperparameter Tunning: Using PCA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DlXBamCrZw7"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V15eNQSFqj73"
      },
      "outputs": [],
      "source": [
        "# Select the features to be used in the model\n",
        "features_temp = ['Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
        "            'Visibility(mi)', 'Wind_Speed(mph)', 'Amenity',\n",
        "            'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop',\n",
        "            'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']\n",
        "\n",
        "# Encode the categorical features using one-hot encoding\n",
        "# X = pd.get_dummies(clean_df[features_temp], columns=['Weather_Condition'])\n",
        "X = clean_df[features_temp]\n",
        "# Extract the target variable\n",
        "y = clean_df['Severity']\n",
        "\n",
        "# Split the data into training and testing sets(80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i59sKn40_Qgv"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie9jDlBx2Y0F"
      },
      "outputs": [],
      "source": [
        "#Intermediate step to address fact that PCA is not scale-invariant\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "# Transform\n",
        "scaled_x_train = scaler.transform(X_train)\n",
        "scaled_x_test = scaler.transform(X_test)\n",
        "\n",
        "#Instantiate and Fit PCA\n",
        "pca = PCA(n_components = 18)\n",
        "pca_x_train = pca.fit(scaled_x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8CPgogf5D6q"
      },
      "outputs": [],
      "source": [
        "#Save the explained variance ratios into variable called \"explained_variance_ratios\"\n",
        "explained_variance_ratios = pca.explained_variance_ratio_\n",
        "\n",
        "#Save the CUMULATIVE explained variance ratios into variable called \"cum_evr\"\n",
        "cum_evr = np.cumsum(explained_variance_ratios)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRcbA5Ss5Gkr"
      },
      "outputs": [],
      "source": [
        "# find optimal num components to use (n) by plotting explained variance ratio\n",
        "plt.plot(np.arange(0, 18), cum_evr)\n",
        "plt.plot(np.arange(0, 18), [0.95]*18)\n",
        "plt.xlabel(\"# of components\")\n",
        "plt.ylabel(\"Proportion of Explained Variance\")\n",
        "plt.title(\"PVE as # of components increases\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbfZ1d81AZYg"
      },
      "outputs": [],
      "source": [
        "# Determine the number of components that explain 95% of the variance\n",
        "optimal_components = np.argmax(cum_evr >= 0.95) + 1\n",
        "\n",
        "# Print the optimal number of components\n",
        "print(f\"The optimal number of components is {optimal_components}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEW7pDTZ_u8n"
      },
      "source": [
        "**Based on the above plot, we can see the optimal number of components is around 66, to get a precise number. The np.argmax() function returns the index of the first occurrence of the maximum value in a given array. In this case, the cumulative variance ratio array contains the cumulative sum of explained variance ratios for each component. In this way, since python's index begins with 0 so we added 1 to get the number that represent the number of components as it starts with 1.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldRsxgR8_t83"
      },
      "outputs": [],
      "source": [
        "#Refit and transform on training with optimal n (as deduced from the previous step)\n",
        "pca = PCA(n_components=optimal_components)\n",
        "X_train_pca = pca.fit_transform(scaled_x_train)\n",
        "\n",
        "#Transform on Testing Set and store it as `X_test_pca`\n",
        "X_test_pca = pca.transform(scaled_x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Tv4b71Kt5gD"
      },
      "source": [
        "### **Logistic Regression with PCA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tpp3kI7t5D8"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn.metrics\n",
        "# Initialize `log_reg_pca` model with default parameters and fit it on the PCA transformed training set\n",
        "log_reg_pca = LogisticRegression(penalty = None, random_state=42, solver = 'saga',max_iter=50000)\n",
        "\n",
        "# Use the model to predict on the PCA transformed test set and save these predictions as `y_pred`\n",
        "log_reg_pca.fit(X_train_pca, y_train)\n",
        "y_pred = log_reg_pca.predict(X_test_pca)\n",
        "# Find the accuracy\n",
        "test_accuracy = sklearn.metrics.accuracy_score(y_pred, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGd7xt5yQurn"
      },
      "outputs": [],
      "source": [
        "print(f\"The accuracy of this model is {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA17AWmwFWcb"
      },
      "source": [
        "### **Logistic Regression (whether an accident is severe or not) with PCA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIf29Vcs9mfX"
      },
      "source": [
        "###**Logistic Regression with tuning: Lasso(L1), Ridge(L2), and ElasticNet(L1&L2)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ysxLb-BEEEn"
      },
      "source": [
        "**LASSO L1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dedgm-0yOJE"
      },
      "outputs": [],
      "source": [
        "#LASSO L1\n",
        "regr1 = LogisticRegression(penalty='l1', solver='saga', max_iter=50000, random_state=42)\n",
        "#fit the model\n",
        "regr1.fit(X_train_pca, y_train)\n",
        "\n",
        "y_pred = regr1.predict(X_test_pca)\n",
        "#compute test accuracy\n",
        "test_accuracy = sklearn.metrics.accuracy_score(y_pred, y_test)\n",
        "print(f\"The test accuracy of this model is {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljzty_sBEKwi"
      },
      "source": [
        "**Ridge L2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6S2XsdrD28B"
      },
      "outputs": [],
      "source": [
        "#Ridge L2\n",
        "regr2 = LogisticRegression(penalty='l2', solver='saga', max_iter=50000, random_state=42)\n",
        "#fit the model\n",
        "regr2.fit(X_train_pca, y_train)\n",
        "\n",
        "y_pred = regr2.predict(X_test_pca)\n",
        "#compute test accuracy\n",
        "test_accuracy = sklearn.metrics.accuracy_score(y_pred, y_test)\n",
        "print(f\"The test accuracy of this model is {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2Z9afuWEpZM"
      },
      "source": [
        "**L1 combined with L2 (Elastic Net)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c58sRAQ9EpuR"
      },
      "outputs": [],
      "source": [
        "#ElasticNet\n",
        "regr3 = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=50000, l1_ratio=0.5, random_state=42)\n",
        "#fit the model\n",
        "regr3.fit(X_train_pca, y_train)\n",
        "\n",
        "y_pred = regr3.predict(X_test_pca)\n",
        "#compute test accuracy\n",
        "test_accuracy = sklearn.metrics.accuracy_score(y_pred, y_test)\n",
        "print(f\"The test accuracy of this model is {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYfrUZm_FDkL"
      },
      "source": [
        "**Conclusion**:\n",
        "\n",
        "By implementing different regularizations, we have found the model has accuracy of 94.52% by using Lasso, Ridge, and elastic net, and the result are basically the same compared to pure logistic regression. This indicates that, in this specific problem, adding regularization does not provide significant improvements in model performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG1v19nDHrRf"
      },
      "source": [
        "## Supervised Learning Models (predicting the severity of an accident)\n",
        "Predicting the severity of accidents using machine learning models can help improve road safety and save lives. Accidents can have different levels of severity, ranging from minor injuries to fatalities, and understanding the factors that contribute to these outcomes can inform prevention strategies and improve emergency response. By analyzing large datasets of accident reports and identifying patterns and risk factors associated with different levels of severity, machine learning models can be trained to accurately predict the severity of future accidents. This information can be used to inform policies and interventions aimed at reducing the risk of accidents and mitigating their impact. Overall, the development of machine learning models for accident severity prediction can have important implications for public safety and welfare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD0vJkX3IQuF"
      },
      "source": [
        "###**Decision tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9xG-laFIfZu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjcm760xIUQQ"
      },
      "outputs": [],
      "source": [
        "# Define features and target\n",
        "X = clean_df[['Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
        "            'Visibility(mi)', 'Wind_Speed(mph)', 'Amenity', 'Bump',\n",
        "            'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
        "            'Roundabout', 'Station', 'Stop',\n",
        "            'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']]\n",
        "y = clean_df['Severity']\n",
        "\n",
        "# Split the data into training and testing sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the decision tree model\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score and Cohen's kappa for the baseline model\n",
        "acc_score = accuracy_score(y_test, y_pred)\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Baseline Model:\")\n",
        "print(\"Accuracy Score:\", acc_score)\n",
        "print(\"Cohen's Kappa Score:\", kappa_score)\n",
        "\n",
        "# Hold one out (90/10 train/test split)\n",
        "X_train_90, X_test_90, y_train_90, y_test_90 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "model.fit(X_train_90, y_train_90)\n",
        "y_pred_90 = model.predict(X_test_90)\n",
        "acc_score_90 = accuracy_score(y_test_90, y_pred_90)\n",
        "kappa_score_90 = cohen_kappa_score(y_test_90, y_pred_90)\n",
        "print(\"\\nHold one out (90/10 train/test split):\")\n",
        "print(\"Accuracy Score:\", acc_score_90)\n",
        "print(\"Cohen's Kappa Score:\", kappa_score_90)\n",
        "\n",
        "# 5-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "acc_score_cv = cv_scores.mean()\n",
        "kappa_score_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n5-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", acc_score_cv)\n",
        "print(\"Cohen's Kappa Score:\", kappa_score_cv)\n",
        "\n",
        "# 10-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=10)\n",
        "acc_score_cv10 = cv_scores.mean()\n",
        "kappa_score_cv10 = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", acc_score_cv10)\n",
        "print(\"Cohen's Kappa Score:\", kappa_score_cv10)\n",
        "\n",
        "#10-fold stratified cross-validation (80/20 train/test split)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model, X, y, cv=skf)\n",
        "acc_score_strat_cv = cv_scores.mean()\n",
        "kappa_score_strat_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold stratified cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", acc_score_strat_cv)\n",
        "print(\"Cohen's Kappa Score:\", kappa_score_strat_cv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7hyDr03Ir42"
      },
      "outputs": [],
      "source": [
        "params_dt = {'criterion': ['gini', 'entropy'], 'max_depth': [None, 10, 20, 30]}\n",
        "grid_dt = GridSearchCV(DecisionTreeClassifier(), params_dt, cv=10, scoring='accuracy')\n",
        "grid_dt.fit(X_train, y_train)\n",
        "print(\"Best Decision Tree parameters:\", grid_dt.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, grid_dt.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLFPZsb3Iote"
      },
      "source": [
        "###**Naïve Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2HNr74qJGGF"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Define the Naive Bayes model\n",
        "model = GaussianNB()\n",
        "\n",
        "# ... (rest of the code is the same as the Decision Tree example, just replace the model definition)\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
        "\n",
        "# Define features and target\n",
        "X = clean_df[['Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
        "            'Visibility(mi)', 'Wind_Speed(mph)', 'Amenity', 'Bump',\n",
        "            'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
        "            'Roundabout', 'Station', 'Stop',\n",
        "            'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']]\n",
        "y = clean_df['Severity']\n",
        "\n",
        "# Split the data into training and testing sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the decision tree model\n",
        "model = GaussianNB()\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score and Cohen's kappa for the baseline model\n",
        "NBacc_score = accuracy_score(y_test, y_pred)\n",
        "NBkappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Baseline Model:\")\n",
        "print(\"Accuracy Score:\", NBacc_score)\n",
        "print(\"Cohen's Kappa Score:\", NBkappa_score)\n",
        "\n",
        "# Hold one out (90/10 train/test split)\n",
        "X_train_90, X_test_90, y_train_90, y_test_90 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "model.fit(X_train_90, y_train_90)\n",
        "y_pred_90 = model.predict(X_test_90)\n",
        "NBacc_score_90 = accuracy_score(y_test_90, y_pred_90)\n",
        "NBkappa_score_90 = cohen_kappa_score(y_test_90, y_pred_90)\n",
        "print(\"\\nHold one out (90/10 train/test split):\")\n",
        "print(\"Accuracy Score:\", NBacc_score_90)\n",
        "print(\"Cohen's Kappa Score:\", NBkappa_score_90)\n",
        "\n",
        "# 5-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "NBacc_score_cv = cv_scores.mean()\n",
        "NBkappa_score_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n5-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", NBacc_score_cv)\n",
        "print(\"Cohen's Kappa Score:\", NBkappa_score_cv)\n",
        "\n",
        "# 10-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=10)\n",
        "NBacc_score_cv10 = cv_scores.mean()\n",
        "NBkappa_score_cv10 = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", NBacc_score_cv10)\n",
        "print(\"Cohen's Kappa Score:\", NBkappa_score_cv10)\n",
        "\n",
        "#10-fold stratified cross-validation (80/20 train/test split)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model, X, y, cv=skf)\n",
        "NBacc_score_strat_cv = cv_scores.mean()\n",
        "NBkappa_score_strat_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold stratified cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", NBacc_score_strat_cv)\n",
        "print(\"Cohen's Kappa Score:\", NBkappa_score_strat_cv)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_lSunu3JVtr"
      },
      "outputs": [],
      "source": [
        "params_nb = {'var_smoothing': [1e-10, 1e-9, 1e-8]}\n",
        "grid_nb = GridSearchCV(GaussianNB(), params_nb, cv=10, scoring='accuracy')\n",
        "grid_nb.fit(X_train, y_train)\n",
        "print(\"Best Naïve Bayes parameters:\", grid_nb.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, grid_nb.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USfZma5OJZdi"
      },
      "source": [
        "###**Logistic regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrZ2_Y8lJmWV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
        "\n",
        "# Define features and target\n",
        "X = clean_df[['Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
        "            'Visibility(mi)', 'Wind_Speed(mph)', 'Amenity', 'Bump',\n",
        "            'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
        "            'Roundabout', 'Station', 'Stop',\n",
        "            'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']]\n",
        "y = clean_df['Severity']\n",
        "\n",
        "# Split the data into training and testing sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the decision tree model with increased max_iter\n",
        "model = LogisticRegression(solver = 'saga', random_state=42, max_iter=10000)\n",
        "\n",
        "# # Define the decision tree model\n",
        "# model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score and Cohen's kappa for the baseline model\n",
        "LRacc_score = accuracy_score(y_test, y_pred)\n",
        "LRkappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Baseline Model:\")\n",
        "print(\"Accuracy Score:\", LRacc_score)\n",
        "print(\"Cohen's Kappa Score:\", LRkappa_score)\n",
        "\n",
        "# Hold one out (90/10 train/test split)\n",
        "X_train_90, X_test_90, y_train_90, y_test_90 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "model.fit(X_train_90, y_train_90)\n",
        "y_pred_90 = model.predict(X_test_90)\n",
        "LRacc_score_90 = accuracy_score(y_test_90, y_pred_90)\n",
        "LRkappa_score_90 = cohen_kappa_score(y_test_90, y_pred_90)\n",
        "print(\"\\nHold one out (90/10 train/test split):\")\n",
        "print(\"Accuracy Score:\", LRacc_score_90)\n",
        "print(\"Cohen's Kappa Score:\", LRkappa_score_90)\n",
        "\n",
        "# 5-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "LRacc_score_cv = cv_scores.mean()\n",
        "LRkappa_score_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n5-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", LRacc_score_cv)\n",
        "print(\"Cohen's Kappa Score:\", LRkappa_score_cv)\n",
        "\n",
        "# 10-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=10)\n",
        "LRacc_score_cv10 = cv_scores.mean()\n",
        "LRkappa_score_cv10 = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", LRacc_score_cv10)\n",
        "print(\"Cohen's Kappa Score:\", LRkappa_score_cv10)\n",
        "\n",
        "#10-fold stratified cross-validation (80/20 train/test split)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model, X, y, cv=skf)\n",
        "LRacc_score_strat_cv = cv_scores.mean()\n",
        "LRkappa_score_strat_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold stratified cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", LRacc_score_strat_cv)\n",
        "print(\"Cohen's Kappa Score:\", LRkappa_score_strat_cv)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zyerbYnMakk"
      },
      "outputs": [],
      "source": [
        "params_lr = {'penalty': ['l1', 'l2']}\n",
        "grid_lr = GridSearchCV(LogisticRegression(max_iter=10000, solver='saga'), params_lr, cv=10, scoring='accuracy')\n",
        "grid_lr.fit(X_train, y_train)\n",
        "print(\"Best Logistic Regression parameters:\", grid_lr.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, grid_lr.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UabZ5nhFKoB7"
      },
      "source": [
        "###**Ramdon forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olO2iSYqKsap"
      },
      "outputs": [],
      "source": [
        "# Define features and target\n",
        "X = clean_df[['Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
        "            'Visibility(mi)', 'Wind_Speed(mph)', 'Amenity', 'Bump',\n",
        "            'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
        "            'Roundabout', 'Station', 'Stop',\n",
        "            'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']]\n",
        "y = clean_df['Severity']\n",
        "# Split the data into training and testing sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the AdaBoost model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score and Cohen's kappa for the baseline model\n",
        "RFacc_score = accuracy_score(y_test, y_pred)\n",
        "RFkappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Baseline Model:\")\n",
        "print(\"Accuracy Score:\", RFacc_score)\n",
        "print(\"Cohen's Kappa Score:\", RFkappa_score)\n",
        "\n",
        "# Hold one out (90/10 train/test split)\n",
        "X_train_90, X_test_90, y_train_90, y_test_90 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "model.fit(X_train_90, y_train_90)\n",
        "y_pred_90 = model.predict(X_test_90)\n",
        "RFacc_score_90 = accuracy_score(y_test_90, y_pred_90)\n",
        "RFkappa_score_90 = cohen_kappa_score(y_test_90, y_pred_90)\n",
        "print(\"\\nHold one out (90/10 train/test split):\")\n",
        "print(\"Accuracy Score:\", RFacc_score_90)\n",
        "print(\"Cohen's Kappa Score:\", RFkappa_score_90)\n",
        "# 5-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "RFacc_score_cv = cv_scores.mean()\n",
        "RFkappa_score_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n5-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", RFacc_score_cv)\n",
        "print(\"Cohen's Kappa Score:\", RFkappa_score_cv)\n",
        "\n",
        "# 10-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=10)\n",
        "RFacc_score_cv10 = cv_scores.mean()\n",
        "RFkappa_score_cv10 = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", RFacc_score_cv10)\n",
        "print(\"Cohen's Kappa Score:\", RFkappa_score_cv10)\n",
        "\n",
        "#10-fold stratified cross-validation (80/20 train/test split)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model, X, y, cv=skf)\n",
        "RFacc_score_strat_cv = cv_scores.mean()\n",
        "RFkappa_score_strat_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold stratified cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", RFacc_score_strat_cv)\n",
        "print(\"Cohen's Kappa Score:\", RFkappa_score_strat_cv)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq-eQTkSKpaN"
      },
      "outputs": [],
      "source": [
        "params_rf = {'criterion': ['gini', 'entropy'], 'max_depth': [10, 20]}\n",
        "grid_rf = GridSearchCV(RandomForestClassifier(), params_rf, cv=10, scoring='accuracy')\n",
        "grid_rf.fit(X_train, y_train)\n",
        "print(\"Best Random Forest parameters:\", grid_rf.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, grid_rf.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmC04mgYNIgF"
      },
      "source": [
        "###**Adaboost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UBHd0dLNM3M"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "\n",
        "# Define features and target\n",
        "X = clean_df[['Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
        "            'Visibility(mi)', 'Wind_Speed(mph)', 'Amenity', 'Bump',\n",
        "            'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
        "            'Roundabout', 'Station', 'Stop',\n",
        "            'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']]\n",
        "y = clean_df['Severity']\n",
        "# Split the data into training and testing sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the AdaBoost model\n",
        "model = AdaBoostClassifier(random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score and Cohen's kappa for the baseline model\n",
        "ABacc_score = accuracy_score(y_test, y_pred)\n",
        "ABkappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Baseline Model:\")\n",
        "print(\"Accuracy Score:\", ABacc_score)\n",
        "print(\"Cohen's Kappa Score:\", ABkappa_score)\n",
        "\n",
        "# Hold one out (90/10 train/test split)\n",
        "X_train_90, X_test_90, y_train_90, y_test_90 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "model.fit(X_train_90, y_train_90)\n",
        "y_pred_90 = model.predict(X_test_90)\n",
        "ABacc_score_90 = accuracy_score(y_test_90, y_pred_90)\n",
        "ABkappa_score_90 = cohen_kappa_score(y_test_90, y_pred_90)\n",
        "print(\"\\nHold one out (90/10 train/test split):\")\n",
        "print(\"Accuracy Score:\", ABacc_score_90)\n",
        "print(\"Cohen's Kappa Score:\", ABkappa_score_90)\n",
        "# 5-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "ABacc_score_cv = cv_scores.mean()\n",
        "ABkappa_score_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n5-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", ABacc_score_cv)\n",
        "print(\"Cohen's Kappa Score:\", ABkappa_score_cv)\n",
        "\n",
        "# 10-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=10)\n",
        "ABacc_score_cv10 = cv_scores.mean()\n",
        "ABkappa_score_cv10 = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", ABacc_score_cv10)\n",
        "print(\"Cohen's Kappa Score:\", ABkappa_score_cv10)\n",
        "\n",
        "#10-fold stratified cross-validation (80/20 train/test split)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model, X, y, cv=skf)\n",
        "ABacc_score_strat_cv = cv_scores.mean()\n",
        "ABkappa_score_strat_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold stratified cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", ABacc_score_strat_cv)\n",
        "print(\"Cohen's Kappa Score:\", ABkappa_score_strat_cv)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9IkbaPdR6L1"
      },
      "outputs": [],
      "source": [
        "params_ab = {'n_estimators': [10, 50, 100], 'learning_rate': [0.1, 0.5, 1]}\n",
        "grid_ab = GridSearchCV(AdaBoostClassifier(), params_ab, cv=10, scoring='accuracy')\n",
        "grid_ab.fit(X_train, y_train)\n",
        "print(\"Best AdaBoost parameters:\", grid_ab.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, grid_ab.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgjp2aR27UL3"
      },
      "source": [
        "## Supervised Learning Models (predicting the possibility of an accident is severe)\n",
        "Using machine learning models to predict the likelihood of a serious accident might enhance safety procedures and lessen the effects of accidents. Accidents can result in major issues including injuries, property damage, and even fatalities. Machine learning algorithms may be taught to precisely forecast the possibility of a catastrophic accident occurring by evaluating massive datasets of accident reports and detecting patterns and risk variables linked with severe accidents. The creation of safety measures and policies, such as bettering road design, traffic management, and emergency response, can be informed by this information in order to decrease the incidence and severity of accidents. Overall, the creation of machine learning models for anticipating the likelihood of serious mishaps can be very advantageous for the welfare and safety of the general population.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYVqonId3gq3"
      },
      "source": [
        "###**Decision tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9DTdPRR3b-h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giQmNV4M2uUP"
      },
      "outputs": [],
      "source": [
        "# Define features and target\n",
        "X = clean_df[['Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
        "            'Visibility(mi)', 'Wind_Speed(mph)', 'Amenity', 'Bump',\n",
        "            'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
        "            'Roundabout', 'Station', 'Stop',\n",
        "            'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']]\n",
        "y = clean_df['severe_accident']\n",
        "\n",
        "# Split the data into training and testing sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the decision tree model\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score and Cohen's kappa for the baseline model\n",
        "acc_score = accuracy_score(y_test, y_pred)\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Baseline Model:\")\n",
        "print(\"Accuracy Score:\", acc_score)\n",
        "print(\"Cohen's Kappa Score:\", kappa_score)\n",
        "\n",
        "# Hold one out (90/10 train/test split)\n",
        "X_train_90, X_test_90, y_train_90, y_test_90 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "model.fit(X_train_90, y_train_90)\n",
        "y_pred_90 = model.predict(X_test_90)\n",
        "Pacc_score_90 = accuracy_score(y_test_90, y_pred_90)\n",
        "Pkappa_score_90 = cohen_kappa_score(y_test_90, y_pred_90)\n",
        "print(\"\\nHold one out (90/10 train/test split):\")\n",
        "print(\"Accuracy Score:\", Pacc_score_90)\n",
        "print(\"Cohen's Kappa Score:\", Pkappa_score_90)\n",
        "\n",
        "# 5-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "Pacc_score_cv = cv_scores.mean()\n",
        "Pkappa_score_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n5-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", Pacc_score_cv)\n",
        "print(\"Cohen's Kappa Score:\", Pkappa_score_cv)\n",
        "\n",
        "# 10-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=10)\n",
        "Pacc_score_cv10 = cv_scores.mean()\n",
        "Pkappa_score_cv10 = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", Pacc_score_cv10)\n",
        "print(\"Cohen's Kappa Score:\", Pkappa_score_cv10)\n",
        "\n",
        "#10-fold stratified cross-validation (80/20 train/test split)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model, X, y, cv=skf)\n",
        "Pacc_score_strat_cv = cv_scores.mean()\n",
        "Pkappa_score_strat_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold stratified cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", Pacc_score_strat_cv)\n",
        "print(\"Cohen's Kappa Score:\", Pkappa_score_strat_cv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Flo_quQl3eXp"
      },
      "outputs": [],
      "source": [
        "params_dt = {'criterion': ['gini', 'entropy'], 'max_depth': [None, 10, 20, 30]}\n",
        "grid_dt = GridSearchCV(DecisionTreeClassifier(), params_dt, cv=10, scoring='accuracy')\n",
        "grid_dt.fit(X_train, y_train)\n",
        "print(\"Best Decision Tree parameters:\", grid_dt.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, grid_dt.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TREwmUpI30dY"
      },
      "source": [
        "###**Naïve Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQLt0Sfn3vga"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Define the Naive Bayes model\n",
        "model = GaussianNB()\n",
        "\n",
        "# ... (rest of the code is the same as the Decision Tree example, just replace the model definition)\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
        "\n",
        "# Define features and target\n",
        "X = clean_df[['Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
        "            'Visibility(mi)', 'Wind_Speed(mph)', 'Amenity', 'Bump',\n",
        "            'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
        "            'Roundabout', 'Station', 'Stop',\n",
        "            'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']]\n",
        "y = clean_df['severe_accident']\n",
        "\n",
        "# Split the data into training and testing sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the decision tree model\n",
        "model = GaussianNB()\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score and Cohen's kappa for the baseline model\n",
        "PNBacc_score = accuracy_score(y_test, y_pred)\n",
        "PNBkappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Baseline Model:\")\n",
        "print(\"Accuracy Score:\", PNBacc_score)\n",
        "print(\"Cohen's Kappa Score:\", PNBkappa_score)\n",
        "\n",
        "# Hold one out (90/10 train/test split)\n",
        "X_train_90, X_test_90, y_train_90, y_test_90 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "model.fit(X_train_90, y_train_90)\n",
        "y_pred_90 = model.predict(X_test_90)\n",
        "PNBacc_score_90 = accuracy_score(y_test_90, y_pred_90)\n",
        "PNBkappa_score_90 = cohen_kappa_score(y_test_90, y_pred_90)\n",
        "print(\"\\nHold one out (90/10 train/test split):\")\n",
        "print(\"Accuracy Score:\", PNBacc_score_90)\n",
        "print(\"Cohen's Kappa Score:\", PNBkappa_score_90)\n",
        "\n",
        "# 5-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "PNBacc_score_cv = cv_scores.mean()\n",
        "PNBkappa_score_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n5-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", PNBacc_score_cv)\n",
        "print(\"Cohen's Kappa Score:\", PNBkappa_score_cv)\n",
        "\n",
        "# 10-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=10)\n",
        "PNBacc_score_cv10 = cv_scores.mean()\n",
        "PNBkappa_score_cv10 = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", PNBacc_score_cv10)\n",
        "print(\"Cohen's Kappa Score:\", PNBkappa_score_cv10)\n",
        "\n",
        "#10-fold stratified cross-validation (80/20 train/test split)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model, X, y, cv=skf)\n",
        "PNBacc_score_strat_cv = cv_scores.mean()\n",
        "PNBkappa_score_strat_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold stratified cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", PNBacc_score_strat_cv)\n",
        "print(\"Cohen's Kappa Score:\", PNBkappa_score_strat_cv)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v74Dcv_-4Wu1"
      },
      "outputs": [],
      "source": [
        "params_nb = {'var_smoothing': [1e-10, 1e-9, 1e-8]}\n",
        "grid_nb = GridSearchCV(GaussianNB(), params_nb, cv=10, scoring='accuracy')\n",
        "grid_nb.fit(X_train, y_train)\n",
        "print(\"Best Naïve Bayes parameters:\", grid_nb.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, grid_nb.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjCZb4qU4feC"
      },
      "source": [
        "###**Logistic regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CN2_FEO4eoA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
        "\n",
        "# Define features and target\n",
        "X = clean_df[['Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
        "            'Visibility(mi)', 'Wind_Speed(mph)', 'Amenity', 'Bump',\n",
        "            'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
        "            'Roundabout', 'Station', 'Stop',\n",
        "            'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']]\n",
        "y = clean_df['severe_accident']\n",
        "\n",
        "# Split the data into training and testing sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the decision tree model with increased max_iter\n",
        "model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# # Define the decision tree model\n",
        "# model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score and Cohen's kappa for the baseline model\n",
        "PLRacc_score = accuracy_score(y_test, y_pred)\n",
        "PLRkappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Baseline Model:\")\n",
        "print(\"Accuracy Score:\", PLRacc_score)\n",
        "print(\"Cohen's Kappa Score:\", PLRkappa_score)\n",
        "\n",
        "# Hold one out (90/10 train/test split)\n",
        "X_train_90, X_test_90, y_train_90, y_test_90 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "model.fit(X_train_90, y_train_90)\n",
        "y_pred_90 = model.predict(X_test_90)\n",
        "PLRacc_score_90 = accuracy_score(y_test_90, y_pred_90)\n",
        "PLRkappa_score_90 = cohen_kappa_score(y_test_90, y_pred_90)\n",
        "print(\"\\nHold one out (90/10 train/test split):\")\n",
        "print(\"Accuracy Score:\", PLRacc_score_90)\n",
        "print(\"Cohen's Kappa Score:\", PLRkappa_score_90)\n",
        "\n",
        "# 5-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "PLRacc_score_cv = cv_scores.mean()\n",
        "PLRkappa_score_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n5-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", PLRacc_score_cv)\n",
        "print(\"Cohen's Kappa Score:\", PLRkappa_score_cv)\n",
        "\n",
        "# 10-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=10)\n",
        "PLRacc_score_cv10 = cv_scores.mean()\n",
        "PLRkappa_score_cv10 = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", PLRacc_score_cv10)\n",
        "print(\"Cohen's Kappa Score:\", PLRkappa_score_cv10)\n",
        "\n",
        "#10-fold stratified cross-validation (80/20 train/test split)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model, X, y, cv=skf)\n",
        "PLRacc_score_strat_cv = cv_scores.mean()\n",
        "PLRkappa_score_strat_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold stratified cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", PLRacc_score_strat_cv)\n",
        "print(\"Cohen's Kappa Score:\", PLRkappa_score_strat_cv)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVOu4WtP4uvY"
      },
      "outputs": [],
      "source": [
        "params_lr = {'penalty': ['l1', 'l2']}\n",
        "grid_lr = GridSearchCV(LogisticRegression(max_iter=1000), params_lr, cv=10, scoring='accuracy')\n",
        "grid_lr.fit(X_train, y_train)\n",
        "print(\"Best Logistic Regression parameters:\", grid_lr.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, grid_lr.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thcbRrcH4rMP"
      },
      "source": [
        "###**Ramdon forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjTa-rl74pZo"
      },
      "outputs": [],
      "source": [
        "# Define features and target\n",
        "X = clean_df[['Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
        "            'Visibility(mi)', 'Wind_Speed(mph)', 'Amenity', 'Bump',\n",
        "            'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
        "            'Roundabout', 'Station', 'Stop',\n",
        "            'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']]\n",
        "y = clean_df['severe_accident']\n",
        "# Split the data into training and testing sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the AdaBoost model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score and Cohen's kappa for the baseline model\n",
        "PRFacc_score = accuracy_score(y_test, y_pred)\n",
        "PRFkappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Baseline Model:\")\n",
        "print(\"Accuracy Score:\", PRFacc_score)\n",
        "print(\"Cohen's Kappa Score:\", PRFkappa_score)\n",
        "\n",
        "# Hold one out (90/10 train/test split)\n",
        "X_train_90, X_test_90, y_train_90, y_test_90 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "model.fit(X_train_90, y_train_90)\n",
        "y_pred_90 = model.predict(X_test_90)\n",
        "PRFacc_score_90 = accuracy_score(y_test_90, y_pred_90)\n",
        "PRFkappa_score_90 = cohen_kappa_score(y_test_90, y_pred_90)\n",
        "print(\"\\nHold one out (90/10 train/test split):\")\n",
        "print(\"Accuracy Score:\", acc_score_90)\n",
        "print(\"Cohen's Kappa Score:\", kappa_score_90)\n",
        "# 5-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "PRFacc_score_cv = cv_scores.mean()\n",
        "PRFkappa_score_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n5-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", PRFacc_score_cv)\n",
        "print(\"Cohen's Kappa Score:\", PRFkappa_score_cv)\n",
        "\n",
        "# 10-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=10)\n",
        "PRFacc_score_cv10 = cv_scores.mean()\n",
        "PRFkappa_score_cv10 = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", PRFacc_score_cv10)\n",
        "print(\"Cohen's Kappa Score:\", PRFkappa_score_cv10)\n",
        "\n",
        "#10-fold stratified cross-validation (80/20 train/test split)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model, X, y, cv=skf)\n",
        "PRFacc_score_strat_cv = cv_scores.mean()\n",
        "PRFkappa_score_strat_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold stratified cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", PRFacc_score_strat_cv)\n",
        "print(\"Cohen's Kappa Score:\", PRFkappa_score_strat_cv)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGK2NMVT5Cwo"
      },
      "outputs": [],
      "source": [
        "params_rf = {'criterion': ['gini', 'entropy'], 'max_depth': [10, 20]}\n",
        "grid_rf = GridSearchCV(RandomForestClassifier(), params_rf, cv=10, scoring='accuracy')\n",
        "grid_rf.fit(X_train, y_train)\n",
        "print(\"Best Random Forest parameters:\", grid_rf.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, grid_rf.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XBAJWKD54J4"
      },
      "source": [
        "###**Adaboost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Task-WGM5Iby"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "\n",
        "# Define features and target\n",
        "X = clean_df[['Temperature(F)', 'Humidity(%)', 'Pressure(in)',\n",
        "            'Visibility(mi)', 'Wind_Speed(mph)', 'Amenity', 'Bump',\n",
        "            'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
        "            'Roundabout', 'Station', 'Stop',\n",
        "            'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']]\n",
        "y = clean_df['severe_accident']\n",
        "# Split the data into training and testing sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the AdaBoost model\n",
        "model = AdaBoostClassifier(random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score and Cohen's kappa for the baseline model\n",
        "PABacc_score = accuracy_score(y_test, y_pred)\n",
        "PABkappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Baseline Model:\")\n",
        "print(\"Accuracy Score:\", PABacc_score)\n",
        "print(\"Cohen's Kappa Score:\", PABkappa_score)\n",
        "\n",
        "# Hold one out (90/10 train/test split)\n",
        "X_train_90, X_test_90, y_train_90, y_test_90 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "model.fit(X_train_90, y_train_90)\n",
        "y_pred_90 = model.predict(X_test_90)\n",
        "PABacc_score_90 = accuracy_score(y_test_90, y_pred_90)\n",
        "PABkappa_score_90 = cohen_kappa_score(y_test_90, y_pred_90)\n",
        "print(\"\\nHold one out (90/10 train/test split):\")\n",
        "print(\"Accuracy Score:\", PABacc_score_90)\n",
        "print(\"Cohen's Kappa Score:\", PABkappa_score_90)\n",
        "# 5-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "PABacc_score_cv = cv_scores.mean()\n",
        "PABkappa_score_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n5-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", PABacc_score_cv)\n",
        "print(\"Cohen's Kappa Score:\", PABkappa_score_cv)\n",
        "\n",
        "# 10-fold cross-validation (80/20 train/test split)\n",
        "cv_scores = cross_val_score(model, X, y, cv=10)\n",
        "PABacc_score_cv10 = cv_scores.mean()\n",
        "PABkappa_score_cv10 = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", PABacc_score_cv10)\n",
        "print(\"Cohen's Kappa Score:\", PABkappa_score_cv10)\n",
        "\n",
        "#10-fold stratified cross-validation (80/20 train/test split)\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model, X, y, cv=skf)\n",
        "PABacc_score_strat_cv = cv_scores.mean()\n",
        "PABkappa_score_strat_cv = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"\\n10-fold stratified cross-validation (80/20 train/test split):\")\n",
        "print(\"Accuracy Score:\", PABacc_score_strat_cv)\n",
        "print(\"Cohen's Kappa Score:\", PABkappa_score_strat_cv)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuPR5CvS6Cuw"
      },
      "outputs": [],
      "source": [
        "params_ab = {'n_estimators': [10, 50, 100], 'learning_rate': [0.1, 0.5, 1]}\n",
        "grid_ab = GridSearchCV(AdaBoostClassifier(), params_ab, cv=10, scoring='accuracy')\n",
        "grid_ab.fit(X_train, y_train)\n",
        "print(\"Best AdaBoost parameters:\", grid_ab.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, grid_ab.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzNN3h-S84Qh"
      },
      "source": [
        "# Part 4: Model Comparsion and Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn6dFtcGOrCM"
      },
      "outputs": [],
      "source": [
        "# To compare the performance of the different models, you can use the same evaluation metrics (accuracy and Cohen's Kappa score) that you have used for the Decision Tree model. After implementing each of the other models (Logistic Regression, Naïve Bayes, Random Forest, and Adaboost) and applying cross-validation, you can summarize the results in a table or plot.\n",
        "\n",
        "# Here's a suggested approach to compare the models:\n",
        "\n",
        "# Create a dictionary or DataFrame to store the results for each model.\n",
        "# For each model, implement the code similar to what you provided for the Decision Tree model.\n",
        "# Store the accuracy and Cohen's Kappa scores for each model in the results dictionary or DataFrame.\n",
        "# After calculating the scores for all models, compare them using the mean values for each evaluation metric.\n",
        "# Choose the model with the highest mean accuracy and/or Cohen's Kappa score as the best-performing model.\n",
        "# Optionally, you can visualize the results in a bar plot or another suitable plot to make the comparison more intuitive.\n",
        "# Here's an example of how to create a dictionary to store the results:\n",
        "\n",
        "results = {\n",
        "    \"Model\": [],\n",
        "    \"Accuracy\": [],\n",
        "    \"Cohen's Kappa\": []\n",
        "}\n",
        "# After computing the accuracy and Cohen's Kappa scores for each model, you can append the results to the dictionary like this:\n",
        "\n",
        "results[\"Model\"].append(\"Decision Tree\")\n",
        "results[\"Accuracy\"].append(acc_score_cv10)  # Use the desired accuracy score (e.g., 10-fold CV)\n",
        "results[\"Cohen's Kappa\"].append(kappa_score_cv10)  # Use the desired Cohen's Kappa score (e.g., 10-fold CV)\n",
        "results[\"Model\"].append(\"Naive Bayes\")\n",
        "results[\"Accuracy\"].append(NBacc_score_cv10)  # Use the desired accuracy score (e.g., 10-fold CV)\n",
        "results[\"Cohen's Kappa\"].append(NBkappa_score_cv10)\n",
        "results[\"Model\"].append(\"Logistic Regression\")\n",
        "results[\"Accuracy\"].append(LRacc_score_cv10)  # Use the desired accuracy score (e.g., 10-fold CV)\n",
        "results[\"Cohen's Kappa\"].append(LRkappa_score_cv10)\n",
        "results[\"Model\"].append(\"Random Forest\")\n",
        "results[\"Accuracy\"].append(RFacc_score_cv10)  # Use the desired accuracy score (e.g., 10-fold CV)\n",
        "results[\"Cohen's Kappa\"].append(RFkappa_score_cv10)\n",
        "results[\"Model\"].append(\"Adaboost\")\n",
        "results[\"Accuracy\"].append(ABacc_score_cv)  # Use the desired accuracy score (e.g., 10-fold CV)\n",
        "results[\"Cohen's Kappa\"].append(ABkappa_score_cv10)\n",
        "# Repeat the same for the other models (Logistic Regression, Naïve Bayes, Random Forest, and Adaboost).\n",
        "\n",
        "# After obtaining the results for all models, convert the dictionary into a DataFrame:\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Then, you can sort the DataFrame based on the evaluation metric of your choice (accuracy or Cohen's Kappa) to find the best-performing model:\n",
        "results_df.sort_values(by=\"Accuracy\", ascending=False, inplace=True)\n",
        "\n",
        "# Finally, visualize the results using a bar plot or any other suitable plot:\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(results_df[\"Model\"], results_df[\"Accuracy\"])\n",
        "plt.xlabel(\"Models\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Comparison - Accuracy on predicting accident severity\")\n",
        "plt.xticks(rotation=90)\n",
        "for i, v in enumerate(results_df[\"Accuracy\"]):\n",
        "    plt.text(i, v, str(round(v, 3)), color='blue', fontweight='bold')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkBFEru2uQng"
      },
      "outputs": [],
      "source": [
        "possibility_results = {\n",
        "    \"Model\": [],\n",
        "    \"Accuracy\": [],\n",
        "    \"Cohen's Kappa\": []\n",
        "}\n",
        "# After computing the accuracy and Cohen's Kappa scores for each model, you can append the results to the dictionary like this:\n",
        "\n",
        "possibility_results[\"Model\"].append(\"Decision Tree\")\n",
        "possibility_results[\"Accuracy\"].append(Pacc_score_cv10)  # Use the desired accuracy score (e.g., 10-fold CV)\n",
        "possibility_results[\"Cohen's Kappa\"].append(Pkappa_score_cv10)  # Use the desired Cohen's Kappa score (e.g., 10-fold CV)\n",
        "possibility_results[\"Model\"].append(\"Naive Bayes\")\n",
        "possibility_results[\"Accuracy\"].append(PNBacc_score_cv10)  # Use the desired accuracy score (e.g., 10-fold CV)\n",
        "possibility_results[\"Cohen's Kappa\"].append(PNBkappa_score_cv10)\n",
        "possibility_results[\"Model\"].append(\"Logistic Regression\")\n",
        "possibility_results[\"Accuracy\"].append(PLRacc_score_cv10)  # Use the desired accuracy score (e.g., 10-fold CV)\n",
        "possibility_results[\"Cohen's Kappa\"].append(PLRkappa_score_cv10)\n",
        "possibility_results[\"Model\"].append(\"Random Forest\")\n",
        "possibility_results[\"Accuracy\"].append(PRFacc_score_cv10)  # Use the desired accuracy score (e.g., 10-fold CV)\n",
        "possibility_results[\"Cohen's Kappa\"].append(PRFkappa_score_cv10)\n",
        "possibility_results[\"Model\"].append(\"Adaboost\")\n",
        "possibility_results[\"Accuracy\"].append(PABacc_score_cv)  # Use the desired accuracy score (e.g., 10-fold CV)\n",
        "possibility_results[\"Cohen's Kappa\"].append(PABkappa_score_cv10)\n",
        "# Repeat the same for the other models (Logistic Regression, Naïve Bayes, Random Forest, and Adaboost).\n",
        "\n",
        "# After obtaining the results for all models, convert the dictionary into a DataFrame:\n",
        "\n",
        "possibility_results_df = pd.DataFrame(possibility_results)\n",
        "\n",
        "# Then, you can sort the DataFrame based on the evaluation metric of your choice (accuracy or Cohen's Kappa) to find the best-performing model:\n",
        "possibility_results_df.sort_values(by=\"Accuracy\", ascending=False, inplace=True)\n",
        "\n",
        "# Finally, visualize the results using a bar plot or any other suitable plot:\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(possibility_results_df[\"Model\"], possibility_results_df[\"Accuracy\"])\n",
        "plt.xlabel(\"Models\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Comparison - Accuracy on predicting the possibility of severe accident\")\n",
        "plt.xticks(rotation=90)\n",
        "for i, v in enumerate(possibility_results_df[\"Accuracy\"]):\n",
        "    plt.text(i, v, str(round(v, 3)), color='blue', fontweight='bold')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ujorwTvNj4S"
      },
      "outputs": [],
      "source": [
        "possibility_results_df#display the accuracy and kappa score of all 5 models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA_sSOvE9MxB"
      },
      "source": [
        "In this project, we want to implement both unsupervised and supervised machine learning models to achieve our research questions. We first cleaned the dataset and ensure the data without any outliers (`accident_time` less than 0) and Null values. We then created a new column called `severe_accident` that can distinguish whether an accident is severe or not. According to our dataset, our two labels are binary variables so the machine learning will be a classification question.\n",
        "\n",
        "We used PCA (unsupervised learning) to reduce dimensional and apply supervised learning models (Logistic Regression with regularizations) after that. We then used supervised laerning methods like decision tree, random forest, SVM, Adaboost and Naive Bayes. We also tunned their hyperparameters to see which one can produce the best accuracy.\n",
        "\n",
        "After using PCA on the feature list, the best model is logistic regression with l2 and elastic net.\n",
        "for the model that predicting the severity of an accident, the best one is random forest, the accuracy is 94.4%.\n",
        "for the model that predicting the possibility of an accident is severe or not, the best one is adaboost with accuracy of 95.446%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOoX6ZJQkwDK"
      },
      "source": [
        "# Part 5 : Challenges and Obstacles Faced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_1SMnhMkykW"
      },
      "source": [
        "* **Missing Data:** There are many missing value/nulls in data, due to collection error or something else. In this way, the cleaning process has eliminated many rows.\n",
        "\n",
        "* **Data size:** This dataset is extremely large and has nearly 3 million rows. There are still nearly 1 million rows after the data preprocessing and cleaning phase. In order to avoid system overload we have to using sampling to select 50000 rows from the dataset but we think those data might be not very representative for the whole picture.\n",
        "\n",
        "* **Select features:** Additionally, we would like to add some features that can make our model better and representative but the colab environment is unable to process too many features in short time. So we have to further drop the number of features and only keep 18 of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWEXWPWVk23D"
      },
      "source": [
        "# Part 6 : Further Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VhKp-7Vk7J0"
      },
      "source": [
        "* We would like to try methods covered in the lecture to replace missing value/Nulls with aggregation or KNN value to retain more rows in training data\n",
        "* We would like to access more powerful system that can process more rows where we can expand the size of our training and testing data. The current environment has very limited RAM which always crash for large dataset."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}